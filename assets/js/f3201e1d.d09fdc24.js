"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[1374],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=c(n),m=a,k=d["".concat(l,".").concat(m)]||d[m]||u[m]||o;return n?r.createElement(k,s(s({ref:t},p),{},{components:n})):r.createElement(k,s({ref:t},p))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,s=new Array(o);s[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:a,s[1]=i;for(var c=2;c<o;c++)s[c]=n[c];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},6645:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var r=n(7462),a=(n(7294),n(3905));const o={id:"quick-start",title:"Quick start",sidebar_position:2},s=void 0,i={unversionedId:"user-guide/quick-start",id:"user-guide/quick-start",title:"Quick start",description:"How to use",source:"@site/docs/user-guide/2-quickstart.md",sourceDirName:"user-guide",slug:"/user-guide/quick-start",permalink:"/docs/user-guide/quick-start",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/user-guide/2-quickstart.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{id:"quick-start",title:"Quick start",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Platform deployment",permalink:"/docs/user-guide/deployment"},next:{title:"Develop Environment",permalink:"/docs/user-guide/development"}},l={},c=[{value:"How to use",id:"how-to-use",level:2},{value:"Deploy DataStream tasks",id:"deploy-datastream-tasks",level:3},{value:"Deploy the FlinkSql task",id:"deploy-the-flinksql-task",level:3},{value:"Task start process",id:"task-start-process",level:3}],p={toc:c};function u(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"how-to-use"},"How to use"),(0,a.kt)("p",null,"The installation of the one-stop platform ",(0,a.kt)("inlineCode",{parentName:"p"},"streampark-console")," has been introduced in detail in the previous chapter. In this chapter, let's see how to quickly deploy and run a job with ",(0,a.kt)("inlineCode",{parentName:"p"},"streampark-console"),". The official structure and specification) and projects developed with ",(0,a.kt)("inlineCode",{parentName:"p"},"streamx")," are well supported. Let's use ",(0,a.kt)("inlineCode",{parentName:"p"},"streamx-quickstart")," to quickly start the journey of ",(0,a.kt)("inlineCode",{parentName:"p"},"streampark-console")),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"streamx-quickstart")," is a sample program for developing Flink by StreamPark. For details, please refer to:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Github: ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/apache/streampark-quickstart.git"},"https://github.com/apache/incubator-streampark-quickstart.git")),(0,a.kt)("li",{parentName:"ul"},"Gitee: ",(0,a.kt)("a",{parentName:"li",href:"https://gitee.com/mirrors_apache/incubator-streampark-quickstart.git"},"https://gitee.com/mirrors_apache/incubator-streampark-quickstart.git"))),(0,a.kt)("h3",{id:"deploy-datastream-tasks"},"Deploy DataStream tasks"),(0,a.kt)("p",null,"The following example demonstrates how to deploy a DataStream application"),(0,a.kt)("video",{src:"http://assets.streamxhub.com/datastream.mp4",controls:"controls",width:"100%",height:"100%"}),(0,a.kt)("h3",{id:"deploy-the-flinksql-task"},"Deploy the FlinkSql task"),(0,a.kt)("p",null,"The following example demonstrates how to deploy a FlinkSql application"),(0,a.kt)("video",{src:"http://assets.streamxhub.com//flinksql.mp4",controls:"controls",width:"100%",height:"100%"}),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The flink sql used in the project demonstration is as follows")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE user_log (\n    user_id VARCHAR,\n    item_id VARCHAR,\n    category_id VARCHAR,\n    behavior VARCHAR,\n    ts TIMESTAMP(3)\n ) WITH (\n'connector.type' = 'kafka', -- Using the kafka connector\n'connector.version' = 'universal',  -- kafka version, universal supports versions above 0.11\n'connector.topic' = 'user_behavior',  -- kafka topic\n'connector.properties.bootstrap.servers'='kafka-1:9092,kafka-2:9092,kafka-3:9092',\n'connector.startup-mode' = 'earliest-offset', -- Read from start offset\n'update-mode' = 'append',\n'format.type' = 'json',  -- The data source format is json\n'format.derive-schema' = 'true' -- Determine json parsing rules from DDL schema\n );\n\nCREATE TABLE pvuv_sink (\n    dt VARCHAR,\n    pv BIGINT,\n    uv BIGINT\n ) WITH (\n'connector.type' = 'jdbc', -- using jdbc connector\n'connector.url' = 'jdbc:mysql://test-mysql:3306/test', -- jdbc url\n'connector.table' = 'pvuv_sink', -- Table Name\n'connector.username' = 'root', -- username\n'connector.password' = '123456', --password\n'connector.write.flush.max-rows' = '1' -- Default 5000, changed to 1 for demonstration\n );\n\nINSERT INTO pvuv_sink\nSELECT\n  DATE_FORMAT(ts, 'yyyy-MM-dd HH:00') dt,\n  COUNT(*) AS pv,\n  COUNT(DISTINCT user_id) AS uv\nFROM user_log\nGROUP BY DATE_FORMAT(ts, 'yyyy-MM-dd HH:00');\n")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The maven dependencies are used as follows")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},"\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.48</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-sql-connector-kafka_2.11</artifactId>\n    <version>1.12.0</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-jdbc_2.11</artifactId>\n    <version>1.12.0</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-json</artifactId>\n    <version>1.12.0</version>\n</dependency>\n\n")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The data sent by Kafka simulation is as follows")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-json"},'\n{"user_id": "543462", "item_id":"1715", "category_id": "1464116", "behavior": "pv", "ts":"2021-02-01T01:00:00Z"}\n{"user_id": "662867", "item_id":"2244074","category_id":"1575622","behavior": "pv", "ts":"2021-02-01T01:00:00Z"}\n{"user_id": "662867", "item_id":"2244074","category_id":"1575622","behavior": "pv", "ts":"2021-02-01T01:00:00Z"}\n{"user_id": "662867", "item_id":"2244074","category_id":"1575622","behavior": "learning flink", "ts":"2021-02-01T01:00:00Z"}\n\n')),(0,a.kt)("h3",{id:"task-start-process"},"Task start process"),(0,a.kt)("p",null,"The task startup flow chart is as follows"),(0,a.kt)("center",null,(0,a.kt)("img",{src:"/doc/image/streamx_start.png"}),(0,a.kt)("br",null),(0,a.kt)("strong",null,"streampark-console submit task process")),(0,a.kt)("p",null,"Regarding the concept of the project, ",(0,a.kt)("inlineCode",{parentName:"p"},"Development Mode"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"savepoint"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"NoteBook"),", custom jar management, task release, task recovery, parameter configuration, parameter comparison, multi-version management and more tutorials and documents will be continuously updated. .."))}u.isMDXComponent=!0}}]);